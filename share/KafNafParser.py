
# coding: utf-8

# In[143]:

#CHRISTOPHER M. CHURCH
#ASSISTANT PROFESSOR OF HISTORY
#UNIVERSITY OF NEVADA, RENO

#VISUALIZING EMPIRE ALPHA

"""
This py file relies upon the KafNafParserPy project (https://github.com/cltl/KafNafParserPy) in order to process 
OpeNER (opener-project.eu) results.

This file will take the KAF (KYOTO ANNOTATION FORMAT) files generated by OpeNER's NER module and export a TSV file. This
TSV file includes the location term and a 31 token KWIC (keyword in context) for all locations where this term shows up.
This is to be exported into OpenRefine for human supervision, to identify those locations that are valid from the KWIC.
Once the results of NER are verified, the total number of "hits" can be tallied for each location term.

The next step would be to use a gazetteer to identify to what these terms correlate.
"""

from KafNafParserPy import KafNafParser
from collections import defaultdict
import operator
import os
import codecs
wd = "I:/Dropbox/NDAD/Visualizing-Empire/OpeNER/Output"
os.chdir(wd)


# In[3]:

#freqdict = defaultdict(int) #create a dictionary to store frequencies, goes with simpledict, also commented out
#for token in my_parser.get_tokens():
#    freqdict[token.get_text()] += 1


# In[153]:

#simpledict ={}


for root,dirs,files in os.walk("./"): #walk through the working directory and look for KAF files
    for f in files:
        if f.endswith('.kaf'):
            output="ENT_ID\tTYPE\tTERM\tKWIC\n"
            print "Reading: ", f
            my_parser = KafNafParser(f) #create the KafNafParser
            for entity in my_parser.get_entities(): #get the entities identified through NER
                if entity.get_type()=="LOCATION": #if it's a location, then let's get its term references
                    entity_type = entity.get_type() #get the entity type
                    entity_id = entity.get_id() #get the entity id
                    ref = entity.get_node().find("references") #find the references node in the KAF file
                    targets = ref.find("span").findall("target") #get a list of all the targets in the references node
                    term_id = targets[0].attrib["id"] #get the first target term id, so we can generate the term text in the next two lines
                    word_id = my_parser.get_term(term_id).get_node().find("span").find("target").attrib["id"] #get the word id
                    word = my_parser.get_token(word_id).get_text() #get the word text

                    #simpledict[word]=freqdict[word] #generate a simple tally that ignores whether each LOCATION is valid; 
                                                     #commented out since OpenRefine will be used to validate


                    sentence = "" #start a blank KWIC sentence variable
                    for target in targets: #iterate through all targets in the references element for the entity node
                        tid = int(my_parser.get_term(target.attrib["id"]).get_node().find("span").find("target").attrib["id"][1:]) #get the token id for the target
                        for x in range(tid-15,tid+15): #iterate through the tokens 15 before and 15 after the target
                            token = my_parser.get_token("w"+str(x)).get_text()
                            if x == tid:
                                token = token.upper() #make all but the target lower case
                            else:
                                token = token.lower() #make the target all uppercase (easier to find)
                            sentence = sentence + token +" " #build the KWIC sentence
                        output = output + entity_id + "\t" + '"' + entity_type + '"' + "\t" + '"' + word + '"' + "\t" + '"' + sentence + '"' + "\n" #add to our output variable which will be saved next

            #save the output
            filename = "kwic/"+f+".KWIC.tsv"
            with codecs.open(filename,'w',encoding='utf8') as f:
                f.write(output)
            f.close()
            print "Saved: ", filename


# In[ ]:


#print "SIMPLIFIED DICTIONARY"
#for key,value in sorted(simpledict.items(),key=operator.itemgetter(1)):
#    print key,value


# In[ ]:

"""
#OTHER FUNCTIONS
my_parser = KafNafParser(kaf_file)
for token in my_parser.get_tokens():
    print 'Token id',token.get_id()
    print 'Token text',token.get_text()

for term_obj in my_parser.get_terms():
    print 'Lemma',term_obj.get_lemma()
    print 'Ids:',term_obj.get_span().get_span_ids()

for prop in my_paser.get_properties():
    print 'Id',prop.get_id()
    for reference in prop.get_references():
        for span_obj in reference: ##Iterator over Creference object
            print 'span ids',span_obj.get_span_ids()
"""

